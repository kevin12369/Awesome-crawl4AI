# Awesome-crawl4AI æµ‹è¯•è§„èŒƒæ–‡æ¡£

> ç‰ˆæœ¬ï¼š1.0.0
> æ›´æ–°æ—¥æœŸï¼š2024-12-25
> ç›®æ ‡ï¼šæ„å»ºé«˜è´¨é‡ã€å¯ç»´æŠ¤çš„æµ‹è¯•ä½“ç³»

---

## ç›®å½•

1. [æµ‹è¯•ç­–ç•¥](#1-æµ‹è¯•ç­–ç•¥)
2. [å•å…ƒæµ‹è¯•è§„èŒƒ](#2-å•å…ƒæµ‹è¯•è§„èŒƒ)
3. [é›†æˆæµ‹è¯•è§„èŒƒ](#3-é›†æˆæµ‹è¯•è§„èŒƒ)
4. [æµ‹è¯•å‘½åå’Œç»„ç»‡](#4-æµ‹è¯•å‘½åå’Œç»„ç»‡)
5. [æµ‹è¯•æœ€ä½³å®è·µ](#5-æµ‹è¯•æœ€ä½³å®è·µ)
6. [CI/CD é›†æˆ](#6-cicd-é›†æˆ)
7. [æµ‹è¯•ç¤ºä¾‹](#7-æµ‹è¯•ç¤ºä¾‹)
8. [æµ‹è¯•å·¥å…·å’Œå‘½ä»¤](#8-æµ‹è¯•å·¥å…·å’Œå‘½ä»¤)

---

## 1. æµ‹è¯•ç­–ç•¥

### 1.1 æµ‹è¯•é‡‘å­—å¡”

æˆ‘ä»¬é‡‡ç”¨ç»å…¸çš„æµ‹è¯•é‡‘å­—å¡”ç­–ç•¥ï¼Œç¡®ä¿åœ¨åˆé€‚çš„å±‚æ¬¡è¿›è¡Œæµ‹è¯•ï¼š

```
        /\
       /  \        ç«¯åˆ°ç«¯æµ‹è¯• (E2E)
      /____\       - 10% æ¯”ä¾‹
     /      \
    /        \      é›†æˆæµ‹è¯•
   /__________\     - 30% æ¯”ä¾‹
  /            \
 /              \   å•å…ƒæµ‹è¯•
/________________\  - 60% æ¯”ä¾‹
```

#### å„å±‚æ¬¡æµ‹è¯•è¯´æ˜

**å•å…ƒæµ‹è¯• (60%)**
- æµ‹è¯•å•ä¸ªå‡½æ•°ã€æ–¹æ³•ã€ç±»çš„è¡Œä¸º
- å¿«é€Ÿæ‰§è¡Œï¼ˆæ¯«ç§’çº§ï¼‰
- æ— å¤–éƒ¨ä¾èµ–ï¼ˆä½¿ç”¨ Mockï¼‰
- é«˜è¦†ç›–ç‡è¦æ±‚

**é›†æˆæµ‹è¯• (30%)**
- æµ‹è¯•æ¨¡å—é—´çš„äº¤äº’
- æµ‹è¯•ä¸å¤–éƒ¨æœåŠ¡çš„é›†æˆ
- é€‚åº¦ä½¿ç”¨çœŸå®ä¾èµ–
- ä¸­ç­‰æ‰§è¡Œé€Ÿåº¦ï¼ˆç§’çº§ï¼‰

**ç«¯åˆ°ç«¯æµ‹è¯• (10%)**
- æµ‹è¯•å®Œæ•´ç”¨æˆ·åœºæ™¯
- ä½¿ç”¨çœŸå®æµè§ˆå™¨å’Œç½‘ç»œ
- è¾ƒæ…¢æ‰§è¡Œï¼ˆåˆ†é’Ÿçº§ï¼‰
- è¦†ç›–å…³é”®ä¸šåŠ¡æµç¨‹

### 1.2 æµ‹è¯•è¦†ç›–ç‡è¦æ±‚

| æ¨¡å—ç±»å‹ | è¦†ç›–ç‡ç›®æ ‡ | è¯´æ˜ |
|---------|-----------|------|
| æ ¸å¿ƒå¼•æ“ | 90%+ | `packages/crawler/` æ ¸å¿ƒé€»è¾‘ |
| æå–å™¨ | 85%+ | `packages/extractors/` æ•°æ®æå– |
| å¤„ç†å™¨ | 85%+ | `packages/processors/` æ•°æ®å¤„ç† |
| é›†æˆæ¨¡å— | 75%+ | `packages/integrations/` ç¬¬ä¸‰æ–¹é›†æˆ |
| å·¥å…·ç±» | 80%+ | è¾…åŠ©å·¥å…·å‡½æ•° |
| **æ•´ä½“ç›®æ ‡** | **80%+** | é¡¹ç›®æ•´ä½“è¦†ç›–ç‡ |

### 1.3 æµ‹è¯•ä¼˜å…ˆçº§

**P0 - å¿…é¡»æµ‹è¯•ï¼ˆå…³é”®åŠŸèƒ½ï¼‰**
- çˆ¬è™«æ ¸å¿ƒå¼•æ“ï¼ˆå¯åŠ¨ã€åœæ­¢ã€çŠ¶æ€ç®¡ç†ï¼‰
- å¼‚æ­¥å¹¶å‘æ§åˆ¶
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- æ•°æ®æå–æ ¸å¿ƒé€»è¾‘

**P1 - åº”è¯¥æµ‹è¯•ï¼ˆé‡è¦åŠŸèƒ½ï¼‰**
- å„ç±»æå–å™¨ï¼ˆCSSã€XPathã€AI æå–ï¼‰
- åçˆ¬è™«æœºåˆ¶
- ç¼“å­˜ç³»ç»Ÿ
- æ—¥å¿—è®°å½•

**P2 - å¯ä»¥æµ‹è¯•ï¼ˆè¾…åŠ©åŠŸèƒ½ï¼‰**
- å‘½ä»¤è¡Œå·¥å…·
- è¾…åŠ©å‡½æ•°
- æ–‡æ¡£ç¤ºä¾‹ä»£ç 

---

## 2. å•å…ƒæµ‹è¯•è§„èŒƒ

### 2.1 pytest åŸºç¡€é…ç½®

é¡¹ç›®ä½¿ç”¨ `pytest` ä½œä¸ºæµ‹è¯•æ¡†æ¶ï¼Œé…ç½®æ–‡ä»¶ `pyproject.toml`ï¼š

```toml
[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-ra",                      # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
    "--strict-markers",         # ä¸¥æ ¼æ ‡è®°æ¨¡å¼
    "--strict-config",          # ä¸¥æ ¼é…ç½®æ¨¡å¼
    "--showlocals",             # å¤±è´¥æ—¶æ˜¾ç¤ºå±€éƒ¨å˜é‡
]
asyncio_mode = "auto"           # è‡ªåŠ¨æ£€æµ‹å¼‚æ­¥æµ‹è¯•
```

### 2.2 å¼‚æ­¥ä»£ç æµ‹è¯•

ä½¿ç”¨ `pytest-asyncio` æµ‹è¯•å¼‚æ­¥ä»£ç ï¼š

```python
import pytest
from packages.crawler.engine import AsyncCrawler

# æ–¹å¼1ï¼šä½¿ç”¨ async def
@pytest.mark.asyncio
async def test_async_crawler_basic():
    """æµ‹è¯•å¼‚æ­¥çˆ¬è™«åŸºæœ¬åŠŸèƒ½"""
    crawler = AsyncCrawler()
    result = await crawler.crawl("https://example.com")
    assert result.status == 200
    assert result.content is not None

# æ–¹å¼2ï¼šä½¿ç”¨ pytest.fixture
@pytest.fixture
async def crawler():
    """æä¾›çˆ¬è™«å®ä¾‹çš„ fixture"""
    crawler = AsyncCrawler()
    yield crawler
    await crawler.close()

@pytest.mark.asyncio
async def test_with_fixture(crawler):
    """ä½¿ç”¨ fixture çš„å¼‚æ­¥æµ‹è¯•"""
    result = await crawler.crawl("https://example.com")
    assert result.success
```

### 2.3 Mock ä½¿ç”¨ç­–ç•¥

ä½¿ç”¨ `unittest.mock` è¿›è¡Œä¾èµ–éš”ç¦»ï¼š

```python
from unittest.mock import Mock, AsyncMock, patch, MagicMock
import pytest
from packages.crawler.http_client import HttpClient

# åŒæ­¥å‡½æ•° Mock
def test_http_client_with_mock():
    """æµ‹è¯•ä½¿ç”¨ Mock çš„ HTTP å®¢æˆ·ç«¯"""
    client = HttpClient()

    # Mock å“åº”
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.text = "<html>Test</html>"
    mock_response.json.return_value = {"data": "test"}

    # Patch å¤–éƒ¨ä¾èµ–
    with patch.object(client, 'request', return_value=mock_response):
        result = client.fetch("https://example.com")
        assert result.status_code == 200
        assert result.text == "<html>Test</html>"

# å¼‚æ­¥å‡½æ•° Mock
@pytest.mark.asyncio
async def test_async_http_client():
    """æµ‹è¯•å¼‚æ­¥ HTTP å®¢æˆ·ç«¯"""
    client = HttpClient()

    # åˆ›å»º AsyncMock
    mock_response = AsyncMock()
    mock_response.status = 200
    mock_response.text = "<html>Async Test</html>"

    with patch.object(client, 'async_fetch', return_value=mock_response):
        result = await client.fetch_async("https://example.com")
        assert result.status == 200
```

### 2.4 Fixture è®¾è®¡æ¨¡å¼

#### åŸºç¡€ Fixture

```python
import pytest
from packages.crawler import WebCrawler
from packages.extractors import CSSExtractor

@pytest.fixture
def crawler():
    """æä¾›çˆ¬è™«å®ä¾‹"""
    crawler = WebCrawler()
    yield crawler
    crawler.close()

@pytest.fixture
def sample_extractor():
    """æä¾›ç¤ºä¾‹æå–å™¨"""
    return CSSExtractor(
        title="h1",
        content="article p",
        metadata=".meta-info"
    )

@pytest.fixture
def sample_html():
    """æä¾›ç¤ºä¾‹ HTML å†…å®¹"""
    return """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <h1>Main Title</h1>
            <article>
                <p>First paragraph</p>
                <p>Second paragraph</p>
            </article>
            <div class="meta-info">
                <span class="author">Author Name</span>
                <time>2024-12-25</time>
            </div>
        </body>
    </html>
    """
```

#### å¸¦å‚æ•°çš„ Fixture

```python
@pytest.fixture
def mock_response():
    """å·¥å‚æ¨¡å¼ fixtureï¼Œåˆ›å»ºä¸åŒçš„ mock å“åº”"""
    def _create_response(status_code, text):
        mock = Mock()
        mock.status_code = status_code
        mock.text = text
        mock.ok = status_code < 400
        return mock
    return _create_response

def test_with_factory_fixture(mock_response):
    """ä½¿ç”¨å·¥å‚ fixture"""
    success_response = mock_response(200, "OK")
    error_response = mock_response(404, "Not Found")

    assert success_response.ok
    assert not error_response.ok
```

#### Fixture ä½œç”¨åŸŸ

```python
# ä½œç”¨åŸŸï¼šfunctionï¼ˆé»˜è®¤ï¼‰ã€classã€moduleã€session
@pytest.fixture(scope="session")
def database():
    """æ•´ä¸ªæµ‹è¯•ä¼šè¯å…±äº«ä¸€ä¸ªæ•°æ®åº“è¿æ¥"""
    db = Database.connect()
    yield db
    db.close()

@pytest.fixture(scope="module")
def config():
    """æ¨¡å—çº§é…ç½®ï¼Œæ¨¡å—å†…æµ‹è¯•å…±äº«"""
    return load_config("test_config.yaml")

@pytest.fixture(scope="class")
class TestClassFixture:
    """ç±»çº§ fixtureï¼Œç±»å†…æ‰€æœ‰æµ‹è¯•æ–¹æ³•å…±äº«"""
    def setup_method(self):
        self.resource = Resource()

    def teardown_method(self):
        self.resource.cleanup()
```

---

## 3. é›†æˆæµ‹è¯•è§„èŒƒ

### 3.1 é›†æˆæµ‹è¯•èŒƒå›´

é›†æˆæµ‹è¯•åº”è¦†ç›–ï¼š

1. **æ¨¡å—é—´äº¤äº’**
   - çˆ¬è™«å¼•æ“ + æå–å™¨
   - æå–å™¨ + å¤„ç†å™¨
   - ç¼“å­˜ç³»ç»Ÿ + æ ¸å¿ƒé€»è¾‘

2. **å¤–éƒ¨æœåŠ¡é›†æˆ**
   - HTTP å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æµ‹è¯•æœåŠ¡å™¨ï¼‰
   - æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆPlaywright/Seleniumï¼‰
   - æ•°æ®åº“è¿æ¥

3. **å®Œæ•´å·¥ä½œæµ**
   - ç«¯åˆ°ç«¯çˆ¬å–æµç¨‹
   - æ•°æ®å¤„ç†ç®¡é“

### 3.2 æµ‹è¯•ç¯å¢ƒå‡†å¤‡

```python
import pytest
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading

@pytest.fixture(scope="session")
def test_server():
    """å¯åŠ¨æµ‹è¯• HTTP æœåŠ¡å™¨"""
    class TestHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            self.send_response(200)
            self.send_header('Content-Type', 'text/html')
            self.end_headers()
            self.wfile.write(b"<html><body>Test Server</body></html>")

    server = HTTPServer(('localhost', 8765), TestHandler)
    thread = threading.Thread(target=server.serve_forever)
    thread.daemon = True
    thread.start()

    yield "http://localhost:8765"

    server.shutdown()
```

### 3.3 å¤–éƒ¨ä¾èµ– Mock ç­–ç•¥

```python
# ä½¿ç”¨ pytest-responsesï¼ˆæ¨èï¼‰
import pytest
import responses

@responses.activate
def test_with_responses():
    """ä½¿ç”¨ responses åº“ Mock HTTP è¯·æ±‚"""
    responses.add(
        responses.GET,
        "https://api.example.com/data",
        json={"result": "success"},
        status=200
    )

    client = ApiClient()
    result = client.fetch_data()
    assert result == {"result": "success"}

# ä½¿ç”¨ aioresponsesï¼ˆå¼‚æ­¥ï¼‰
import pytest
import aioresponses

@pytest.mark.asyncio
async def test_async_with_aioresponses():
    """æµ‹è¯•å¼‚æ­¥ HTTP å®¢æˆ·ç«¯"""
    with aioresponses.aioresponses() as m:
        m.get("https://api.example.com/data", payload={"result": "success"})

        client = AsyncApiClient()
        result = await client.fetch_data()
        assert result == {"result": "success"}
```

### 3.4 æµ‹è¯•æ•°æ®ç®¡ç†

```python
import pytest
import json
from pathlib import Path

@pytest.fixture
def test_data_dir():
    """æµ‹è¯•æ•°æ®ç›®å½•"""
    return Path(__file__).parent / "data"

@pytest.fixture
def sample_pages(test_data_dir):
    """åŠ è½½ç¤ºä¾‹ç½‘é¡µæ•°æ®"""
    data_file = test_data_dir / "sample_pages.json"
    with open(data_file, 'r', encoding='utf-8') as f:
        return json.load(f)

# ä½¿ç”¨ pytest-datafiles æ’ä»¶
@pytest.fixture
def html_files(datafiles):
    """è‡ªåŠ¨åŠ è½½æµ‹è¯•æ–‡ä»¶"""
    return {
        'page1': (datafiles / "page1.html").read_text(),
        'page2': (datafiles / "page2.html").read_text()
    }
```

---

## 4. æµ‹è¯•å‘½åå’Œç»„ç»‡

### 4.1 æµ‹è¯•æ–‡ä»¶å‘½å

éµå¾ªä»¥ä¸‹å‘½åè§„èŒƒï¼š

```
tests/
â”œâ”€â”€ test_crawler.py              # æ¨¡å—çº§æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ test_extractors/
â”‚   â”œâ”€â”€ test_css_extractor.py    # å…·ä½“ç»„ä»¶æµ‹è¯•
â”‚   â”œâ”€â”€ test_xpath_extractor.py
â”‚   â””â”€â”€ test_ai_extractor.py
â”œâ”€â”€ test_integration/
â”‚   â”œâ”€â”€ test_crawler_extractor_integration.py
â”‚   â””â”€â”€ test_browser_integration.py
â””â”€â”€ test_e2e/
    â”œâ”€â”€ test_full_workflow.py
    â””â”€â”€ test_batch_crawling.py
```

**å‘½åè§„åˆ™**ï¼š
- æ–‡ä»¶åï¼š`test_<module_name>.py`
- ç±»åï¼š`Test<ClassName>`
- æ–¹æ³•åï¼š`test_<function_name>_<scenario>`

### 4.2 Given-When-Then å‘½åæ¨¡å¼

```python
class TestCSSExtractor:
    """æµ‹è¯• CSS æå–å™¨"""

    def test_extract_title_from_simple_html(self):
        """
        Given: ç®€å•çš„ HTML ç»“æ„
        When: ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–æ ‡é¢˜
        Then: æ­£ç¡®è¿”å›æ ‡é¢˜å†…å®¹
        """
        # Given - å‡†å¤‡æµ‹è¯•æ•°æ®
        html = "<html><body><h1>Test Title</h1></body></html>"
        extractor = CSSExtractor(title="h1")

        # When - æ‰§è¡Œæµ‹è¯•æ“ä½œ
        result = extractor.extract(html)

        # Then - éªŒè¯ç»“æœ
        assert result.title == "Test Title"

    def test_extract_content_when_article_has_multiple_paragraphs(self):
        """
        Given: åŒ…å«å¤šä¸ªæ®µè½çš„æ–‡ç« 
        When: æå–æ–‡ç« å†…å®¹
        Then: è¿”å›æ‰€æœ‰æ®µè½å†…å®¹
        """
        html = """
        <article>
            <p>First paragraph</p>
            <p>Second paragraph</p>
            <p>Third paragraph</p>
        </article>
        """
        extractor = CSSExtractor(content="article p")

        result = extractor.extract(html)

        assert len(result.content) == 3
        assert result.content[0] == "First paragraph"
```

### 4.3 æµ‹è¯•ç›®å½•ç»“æ„

```
tests/
â”œâ”€â”€ unit/                       # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_crawler/
â”‚   â”‚   â”œâ”€â”€ test_engine.py
â”‚   â”‚   â”œâ”€â”€ test_scheduler.py
â”‚   â”‚   â””â”€â”€ test_retry.py
â”‚   â”œâ”€â”€ test_extractors/
â”‚   â”‚   â”œâ”€â”€ test_css_extractor.py
â”‚   â”‚   â”œâ”€â”€ test_xpath_extractor.py
â”‚   â”‚   â””â”€â”€ test_llm_extractor.py
â”‚   â””â”€â”€ test_processors/
â”‚       â”œâ”€â”€ test_cleaner.py
â”‚       â””â”€â”€ test_formatter.py
â”œâ”€â”€ integration/                # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_crawler_extractor.py
â”‚   â”œâ”€â”€ test_cache_integration.py
â”‚   â””â”€â”€ test_browser_integration.py
â”œâ”€â”€ e2e/                        # ç«¯åˆ°ç«¯æµ‹è¯•
â”‚   â”œâ”€â”€ test_full_workflow.py
â”‚   â”œâ”€â”€ test_batch_crawling.py
â”‚   â””â”€â”€ test_real_websites.py
â”œâ”€â”€ fixtures/                   # å…±äº« fixtures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler_fixtures.py
â”‚   â””â”€â”€ data_fixtures.py
â”œâ”€â”€ data/                       # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ html/
â”‚   â”œâ”€â”€ responses/
â”‚   â””â”€â”€ expected_results/
â”œâ”€â”€ conftest.py                 # å…¨å±€ fixtures
â””â”€â”€ __init__.py
```

### 4.4 æµ‹è¯•åˆ†ç»„å’Œæ ‡è®°

```python
import pytest

# å®šä¹‰æ ‡è®°
pytestmark = [
    pytest.mark.unit,           # å•å…ƒæµ‹è¯•
    pytest.mark.fast,           # å¿«é€Ÿæµ‹è¯•
]

# ç±»çº§åˆ«æ ‡è®°
@pytest.mark.integration
class TestCrawlerIntegration:
    """é›†æˆæµ‹è¯•ç±»"""

    @pytest.mark.slow
    def test_crawl_large_website(self):
        """æ…¢é€Ÿæµ‹è¯•ï¼ˆçˆ¬å–å¤§å‹ç½‘ç«™ï¼‰"""
        pass

    @pytest.mark.network
    def test_with_real_network(self):
        """éœ€è¦çœŸå®ç½‘ç»œçš„æµ‹è¯•"""
        pass

# å‡½æ•°çº§åˆ«æ ‡è®°
@pytest.mark.asyncio
@pytest.mark.browser
async def test_with_playwright():
    """éœ€è¦æµè§ˆå™¨çš„å¼‚æ­¥æµ‹è¯•"""
    pass
```

**è¿è¡Œç‰¹å®šæ ‡è®°çš„æµ‹è¯•**ï¼š

```bash
# åªè¿è¡Œå¿«é€Ÿæµ‹è¯•
pytest -m fast

# æ’é™¤æ…¢é€Ÿæµ‹è¯•
pytest -m "not slow"

# è¿è¡Œé›†æˆæµ‹è¯•
pytest -m integration

# è¿è¡Œéœ€è¦ç½‘ç»œçš„æµ‹è¯•
pytest -m "network and not slow"
```

---

## 5. æµ‹è¯•æœ€ä½³å®è·µ

### 5.1 AAA æ¨¡å¼ï¼ˆArrange-Act-Assertï¼‰

```python
def test_user_login_with_valid_credentials():
    """æµ‹è¯•ç”¨æˆ·ç™»å½•"""
    # Arrangeï¼ˆå‡†å¤‡ï¼‰ï¼šè®¾ç½®æµ‹è¯•æ•°æ®å’Œç¯å¢ƒ
    user = User(username="testuser", password="password123")
    login_service = LoginService()

    # Actï¼ˆæ‰§è¡Œï¼‰ï¼šè°ƒç”¨è¢«æµ‹è¯•çš„åŠŸèƒ½
    result = login_service.login(user.username, user.password)

    # Assertï¼ˆæ–­è¨€ï¼‰ï¼šéªŒè¯ç»“æœ
    assert result.success is True
    assert result.token is not None
    assert result.user_id == user.id
```

### 5.2 æµ‹è¯•ç‹¬ç«‹æ€§

```python
# âŒ é”™è¯¯ï¼šæµ‹è¯•ä¹‹é—´æœ‰ä¾èµ–
class TestBadExample:
    def test_step1_create_user(self):
        self.user = create_user("test")

    def test_step2_update_user(self):
        self.user.name = "updated"  # ä¾èµ– test_step1

# âœ… æ­£ç¡®ï¼šæ¯ä¸ªæµ‹è¯•ç‹¬ç«‹
class TestGoodExample:
    def test_create_user(self):
        user = create_user("test")
        assert user.id is not None

    def test_update_user(self):
        user = create_user("test")  # è‡ªå·±åˆ›å»ºæ•°æ®
        user.name = "updated"
        assert user.name == "updated"
```

### 5.3 è¾¹ç•Œæ¡ä»¶æµ‹è¯•

```python
class TestListProcessor:
    """æµ‹è¯•åˆ—è¡¨å¤„ç†å™¨"""

    @pytest.mark.parametrize("input_list,expected", [
        ([], []),                           # ç©ºåˆ—è¡¨
        ([1], [1]),                         # å•å…ƒç´ 
        ([1, 2, 3], [1, 2, 3]),            # æ­£å¸¸æƒ…å†µ
        ([None, 1, None], [1]),            # åŒ…å« None
        (list(range(1000)), list(range(1000))),  # å¤§æ•°æ®é‡
    ])
    def test_process_various_inputs(self, input_list, expected):
        """æµ‹è¯•å„ç§è¾¹ç•Œæ¡ä»¶"""
        processor = ListProcessor()
        result = processor.process(input_list)
        assert result == expected

    def test_process_with_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        processor = StringProcessor()
        result = processor.clean("!@#$%^&*()")
        assert result == ""

    def test_process_with_unicode(self):
        """æµ‹è¯• Unicode å­—ç¬¦"""
        processor = StringProcessor()
        result = processor.clean("ä½ å¥½ä¸–ç•ŒğŸŒ")
        assert "ä½ å¥½" in result
```

### 5.4 å¼‚å¸¸æƒ…å†µæµ‹è¯•

```python
import pytest
from packages.crawler.exceptions import CrawlerError, TimeoutError

def test_crawler_with_invalid_url():
    """æµ‹è¯•æ— æ•ˆ URL"""
    crawler = WebCrawler()
    with pytest.raises(ValueError) as exc_info:
        crawler.crawl("not-a-valid-url")
    assert "Invalid URL" in str(exc_info.value)

def test_crawler_timeout():
    """æµ‹è¯•è¶…æ—¶å¤„ç†"""
    crawler = WebCrawler(timeout=1)
    with pytest.raises(TimeoutError):
        crawler.crawl("https://slow-website.com")

@pytest.mark.asyncio
async def test_async_crawler_network_error():
    """æµ‹è¯•ç½‘ç»œé”™è¯¯"""
    crawler = AsyncCrawler()

    with patch.object(crawler, '_fetch', side_effect=ConnectionError):
        with pytest.raises(CrawlerError) as exc_info:
            await crawler.crawl("https://unreachable.com")
        assert "network" in str(exc_info.value).lower()
```

### 5.5 æ€§èƒ½æµ‹è¯•åŸºç¡€

```python
import time
import pytest

@pytest.mark.benchmark
class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""

    def test_crawl_performance_small_page(self):
        """æµ‹è¯•å°é¡µé¢çˆ¬å–æ€§èƒ½"""
        crawler = WebCrawler()
        start_time = time.time()

        result = crawler.crawl("https://example.com")

        elapsed = time.time() - start_time
        assert result.success
        assert elapsed < 2.0  # åº”è¯¥åœ¨ 2 ç§’å†…å®Œæˆ

    @pytest.mark.parametrize("concurrency", [1, 5, 10, 20])
    def test_concurrent_crawling_performance(self, concurrency):
        """æµ‹è¯•å¹¶å‘çˆ¬å–æ€§èƒ½"""
        crawler = WebCrawler(max_concurrency=concurrency)
        urls = ["https://example.com"] * 10

        start_time = time.time()
        results = crawler.crawl_batch(urls)
        elapsed = time.time() - start_time

        assert all(r.success for r in results)
        # å¹¶å‘åº”è¯¥æ¯”é¡ºåºå¿«
        assert elapsed < 10 * 0.5  # å‡è®¾æ¯ä¸ªè¯·æ±‚ 0.5 ç§’
```

### 5.6 å‚æ•°åŒ–æµ‹è¯•

```python
@pytest.mark.parametrize("url,status,expected_title", [
    ("https://example.com", 200, "Example Domain"),
    ("https://example.org", 200, "Example Organization"),
    ("https://example.net", 200, "Example Network"),
])
def test_crawl_various_websites(url, status, expected_title):
    """å‚æ•°åŒ–æµ‹è¯•å¤šä¸ªç½‘ç«™"""
    crawler = WebCrawler()
    result = crawler.crawl(url)

    assert result.status_code == status
    assert expected_title in result.title

# ç»„åˆå‚æ•°åŒ–
@pytest.mark.parametrize("url", ["https://site1.com", "https://site2.com"])
@pytest.mark.parametrize("format", ["markdown", "html", "json"])
def test_crawl_with_different_formats(url, format):
    """æµ‹è¯• URL å’Œè¾“å‡ºæ ¼å¼çš„ç»„åˆ"""
    crawler = WebCrawler()
    result = crawler.crawl(url, output_format=format)

    assert result.success
    assert result.format == format
```

---

## 6. CI/CD é›†æˆ

### 6.1 GitHub Actions å·¥ä½œæµ

åˆ›å»º `.github/workflows/test.yml`ï¼š

```yaml
name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.9", "3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Lint with flake8
      run: |
        flake8 packages tests --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 packages tests --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics

    - name: Type check with mypy
      run: mypy packages

    - name: Run tests with pytest
      run: |
        pytest --cov=packages --cov-report=xml --cov-report=html --cov-report=term

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Archive coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report-${{ matrix.os }}-${{ matrix.python-version }}
        path: htmlcov/
```

### 6.2 æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

#### pytest-html æ’ä»¶

```bash
pip install pytest-html
```

```bash
pytest --html=report.html --self-contained-html
```

#### allure æŠ¥å‘Š

```bash
pip install allure-pytest

pytest --alluredir=allure-results
allure generate allure-results -o allure-report
```

### 6.3 è¦†ç›–ç‡æŠ¥å‘Š

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=packages --cov-report=html --cov-report=term

# æŸ¥çœ‹è¦†ç›–ç‡é˜ˆå€¼
pytest --cov=packages --cov-fail-under=80
```

### 6.4 æµ‹è¯•å¤±è´¥å¤„ç†

```python
# conftest.py

def pytest_configure(config):
    """é…ç½® pytest"""
    marker_map = {
        "slow": "æ ‡è®°æ…¢é€Ÿæµ‹è¯•",
        "network": "éœ€è¦ç½‘ç»œè¿æ¥",
        "browser": "éœ€è¦æµè§ˆå™¨",
    }

    for marker, description in marker_map.items():
        config.addinivalue_line("markers", f"{marker}: {description}")

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """åœ¨æ¯ä¸ªæµ‹è¯•æ‰§è¡Œåç”ŸæˆæŠ¥å‘Š"""
    outcome = yield
    report = outcome.get_result()

    if report.when == "call":
        # è®°å½•æµ‹è¯•å¤±è´¥ä¿¡æ¯
        if report.failed:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {item.nodeid}")
            print(f"   ä½ç½®: {item.location}")
        else:
            print(f"âœ… æµ‹è¯•é€šè¿‡: {item.nodeid}")
```

---

## 7. æµ‹è¯•ç¤ºä¾‹

### 7.1 å•å…ƒæµ‹è¯•å®Œæ•´ç¤ºä¾‹

```python
"""
test_css_extractor.py
æµ‹è¯• CSS æå–å™¨çš„å•å…ƒæµ‹è¯•
"""
import pytest
from packages.extractors.css_extractor import CSSExtractor
from packages.extractors.exceptions import ExtractionError


class TestCSSExtractorInit:
    """æµ‹è¯• CSSExtractor åˆå§‹åŒ–"""

    def test_init_with_valid_selectors(self):
        """æµ‹è¯•ä½¿ç”¨æœ‰æ•ˆçš„é€‰æ‹©å™¨åˆå§‹åŒ–"""
        extractor = CSSExtractor(
            title="h1",
            content="article p",
            metadata=".metadata"
        )
        assert extractor.title_selector == "h1"
        assert extractor.content_selector == "article p"

    def test_init_with_empty_selector(self):
        """æµ‹è¯•ç©ºé€‰æ‹©å™¨"""
        extractor = CSSExtractor(title="")
        assert extractor.title_selector == ""

    def test_init_with_invalid_type(self):
        """æµ‹è¯•æ— æ•ˆç±»å‹"""
        with pytest.raises(TypeError):
            CSSExtractor(title=123)


class TestCSSExtractorExtract:
    """æµ‹è¯• CSSExtractor æå–åŠŸèƒ½"""

    @pytest.fixture
    def sample_html(self):
        """æä¾›ç¤ºä¾‹ HTML"""
        return """
        <html>
            <body>
                <h1>Main Title</h1>
                <article>
                    <p>First paragraph</p>
                    <p>Second paragraph</p>
                </article>
                <div class="metadata">
                    <span class="author">Author Name</span>
                    <time>2024-12-25</time>
                </div>
            </body>
        </html>
        """

    @pytest.fixture
    def extractor(self):
        """æä¾›æå–å™¨å®ä¾‹"""
        return CSSExtractor(
            title="h1",
            content="article p",
            metadata=".metadata"
        )

    def test_extract_title_successfully(self, sample_html, extractor):
        """Given: åŒ…å«æ ‡é¢˜çš„ HTML
        When: æå–æ ‡é¢˜
        Then: è¿”å›æ­£ç¡®çš„æ ‡é¢˜æ–‡æœ¬
        """
        result = extractor.extract(sample_html)

        assert result.title == "Main Title"

    def test_extract_content_as_list(self, sample_html, extractor):
        """Given: åŒ…å«å¤šä¸ªæ®µè½çš„ HTML
        When: æå–å†…å®¹
        Then: è¿”å›æ®µè½åˆ—è¡¨
        """
        result = extractor.extract(sample_html)

        assert len(result.content) == 2
        assert result.content[0] == "First paragraph"
        assert result.content[1] == "Second paragraph"

    def test_extract_metadata(self, sample_html, extractor):
        """Given: åŒ…å«å…ƒæ•°æ®çš„ HTML
        When: æå–å…ƒæ•°æ®
        Then: è¿”å›å…ƒæ•°æ®å­—å…¸
        """
        result = extractor.extract(sample_html)

        assert result.metadata is not None
        assert "author" in result.metadata
        assert result.metadata["author"] == "Author Name"

    def test_extract_with_empty_html(self, extractor):
        """Given: ç©º HTML å­—ç¬¦ä¸²
        When: æ‰§è¡Œæå–
        Then: æŠ›å‡º ExtractionError
        """
        with pytest.raises(ExtractionError):
            extractor.extract("")

    def test_extract_with_no_matching_elements(self):
        """Given: æ²¡æœ‰åŒ¹é…å…ƒç´ çš„ HTML
        When: æ‰§è¡Œæå–
        Then: è¿”å›ç©ºç»“æœ
        """
        html = "<html><body><p>No title here</p></body></html>"
        extractor = CSSExtractor(title="h1")

        result = extractor.extract(html)

        assert result.title is None

    @pytest.mark.parametrize("html,expected_title", [
        ("<h1>Title1</h1>", "Title1"),
        ("<h1>Title2</h1>", "Title2"),
        ("<h1>  Title3  </h1>", "Title3"),  # å»é™¤ç©ºæ ¼
    ])
    def test_extract_various_titles(self, html, expected_title):
        """å‚æ•°åŒ–æµ‹è¯•å„ç§æ ‡é¢˜"""
        extractor = CSSExtractor(title="h1")
        result = extractor.extract(html)

        assert result.title == expected_title


class TestCSSExtractorEdgeCases:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""

    def test_extract_with_malformed_html(self):
        """æµ‹è¯•æ ¼å¼é”™è¯¯çš„ HTML"""
        malformed_html = "<h1>Title</p><div>Content</div>"
        extractor = CSSExtractor(title="h1", content="div")

        result = extractor.extract(malformed_html)

        assert result.title == "Title"
        assert result.content == ["Content"]

    def test_extract_with_unicode_content(self):
        """æµ‹è¯• Unicode å†…å®¹"""
        html = "<h1>ä½ å¥½ä¸–ç•Œ ğŸŒ</h1><p>æµ‹è¯•å†…å®¹</p>"
        extractor = CSSExtractor(title="h1", content="p")

        result = extractor.extract(html)

        assert result.title == "ä½ å¥½ä¸–ç•Œ ğŸŒ"
        assert result.content == ["æµ‹è¯•å†…å®¹"]

    def test_extract_with_nested_elements(self):
        """æµ‹è¯•åµŒå¥—å…ƒç´ """
        html = """
        <article>
            <div>
                <p>Nested paragraph 1</p>
            </div>
            <p>Nested paragraph 2</p>
        </article>
        """
        extractor = CSSExtractor(content="article p")

        result = extractor.extract(html)

        assert len(result.content) == 2
```

### 7.2 å¼‚æ­¥æµ‹è¯•ç¤ºä¾‹

```python
"""
test_async_crawler.py
æµ‹è¯•å¼‚æ­¥çˆ¬è™«
"""
import pytest
import asyncio
from unittest.mock import AsyncMock, patch
from packages.crawler.async_crawler import AsyncCrawler
from packages.crawler.models import CrawlResult


@pytest.mark.asyncio
class TestAsyncCrawlerBasic:
    """æµ‹è¯•å¼‚æ­¥çˆ¬è™«åŸºç¡€åŠŸèƒ½"""

    async def test_crawl_single_url(self):
        """æµ‹è¯•çˆ¬å–å•ä¸ª URL"""
        crawler = AsyncCrawler()

        with patch.object(crawler, '_fetch', return_value="<html>Test</html>"):
            result = await crawler.crawl("https://example.com")

            assert result.success
            assert result.url == "https://example.com"
            assert result.content == "<html>Test</html>"

    async def test_crawl_with_timeout(self):
        """æµ‹è¯•è¶…æ—¶å¤„ç†"""
        crawler = AsyncCrawler(timeout=0.1)

        async def slow_fetch(*args, **kwargs):
            await asyncio.sleep(1)
            return "<html>Slow</html>"

        with patch.object(crawler, '_fetch', side_effect=slow_fetch):
            with pytest.raises(asyncio.TimeoutError):
                await crawler.crawl("https://slow-site.com")

    async def test_crawl_with_retry(self):
        """æµ‹è¯•é‡è¯•æœºåˆ¶"""
        crawler = AsyncCrawler(max_retries=3)
        call_count = 0

        async def flaky_fetch(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count < 3:
                raise ConnectionError("Network error")
            return "<html>Success</html>"

        with patch.object(crawler, '_fetch', side_effect=flaky_fetch):
            result = await crawler.crawl("https://flaky-site.com")

            assert result.success
            assert call_count == 3


@pytest.mark.asyncio
class TestAsyncCrawlerBatch:
    """æµ‹è¯•æ‰¹é‡å¼‚æ­¥çˆ¬å–"""

    async def test_crawl_multiple_urls(self):
        """æµ‹è¯•çˆ¬å–å¤šä¸ª URL"""
        crawler = AsyncCrawler(max_concurrency=3)
        urls = [
            "https://site1.com",
            "https://site2.com",
            "https://site3.com"
        ]

        async def mock_fetch(url):
            return f"<html>{url}</html>"

        with patch.object(crawler, '_fetch', side_effect=mock_fetch):
            results = await crawler.crawl_batch(urls)

            assert len(results) == 3
            assert all(r.success for r in results)
            assert results[0].url == urls[0]

    async def test_crawl_batch_with_partial_failures(self):
        """æµ‹è¯•æ‰¹é‡çˆ¬å–éƒ¨åˆ†å¤±è´¥"""
        crawler = AsyncCrawler()
        urls = ["https://success.com", "https://fail.com", "https://success2.com"]

        async def selective_fetch(url):
            if "fail" in url:
                raise ConnectionError("Failed")
            return f"<html>{url}</html>"

        with patch.object(crawler, '_fetch', side_effect=selective_fetch):
            results = await crawler.crawl_batch(urls)

            assert len(results) == 3
            assert results[0].success
            assert not results[1].success
            assert results[2].success


@pytest.mark.asyncio
@pytest.mark.slow
class TestAsyncCrawlerRealWorld:
    """çœŸå®ä¸–ç•Œçš„å¼‚æ­¥çˆ¬è™«æµ‹è¯•ï¼ˆæ…¢é€Ÿï¼‰"""

    async def test_crawl_with_rate_limiting(self):
        """æµ‹è¯•é€Ÿç‡é™åˆ¶"""
        crawler = AsyncCrawler(rate_limit=2)  # æ¯ç§’ 2 ä¸ªè¯·æ±‚
        urls = [f"https://example.com/page{i}" for i in range(5)]

        start_time = asyncio.get_event_loop().time()

        with patch.object(crawler, '_fetch', return_value="<html>Test</html>"):
            await crawler.crawl_batch(urls)

        elapsed = asyncio.get_event_loop().time() - start_time
        assert elapsed >= 2.0  # è‡³å°‘éœ€è¦ 2 ç§’
```

### 7.3 Mock ä½¿ç”¨ç¤ºä¾‹

```python
"""
test_http_client.py
æµ‹è¯• HTTP å®¢æˆ·ç«¯ï¼ˆåŒ…å«å„ç§ Mock åœºæ™¯ï¼‰
"""
import pytest
from unittest.mock import Mock, patch, MagicMock
from packages.crawler.http_client import HttpClient
from packages.crawler.exceptions import HttpError


class TestHttpClientWithMock:
    """ä½¿ç”¨ Mock æµ‹è¯• HTTP å®¢æˆ·ç«¯"""

    def test_get_request_success(self):
        """æµ‹è¯•æˆåŠŸçš„ GET è¯·æ±‚"""
        client = HttpClient()

        # åˆ›å»º mock å“åº”
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.text = "<html>Success</html>"
        mock_response.json.return_value = {"status": "ok"}

        # ä½¿ç”¨ patch æ›¿æ¢ requests.get
        with patch('requests.get', return_value=mock_response) as mock_get:
            result = client.get("https://example.com")

            # éªŒè¯è°ƒç”¨
            mock_get.assert_called_once_with(
                "https://example.com",
                headers=None,
                timeout=30
            )

            # éªŒè¯ç»“æœ
            assert result.status_code == 200
            assert result.text == "<html>Success</html>"

    def test_post_request_with_data(self):
        """æµ‹è¯• POST è¯·æ±‚"""
        client = HttpClient()

        mock_response = Mock()
        mock_response.status_code = 201
        mock_response.json.return_value = {"id": 123}

        with patch('requests.post', return_value=mock_response) as mock_post:
            result = client.post(
                "https://api.example.com/data",
                json={"name": "test"}
            )

            mock_post.assert_called_once()
            assert result.status_code == 201
            assert result.json()["id"] == 123

    def test_request_with_retry_logic(self):
        """æµ‹è¯•é‡è¯•é€»è¾‘"""
        client = HttpClient(max_retries=3)

        # ç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œç¬¬äºŒæ¬¡æˆåŠŸ
        fail_response = Mock()
        fail_response.status_code = 500

        success_response = Mock()
        success_response.status_code = 200
        success_response.text = "<html>Success</html>"

        with patch('requests.get', side_effect=[fail_response, success_response]):
            result = client.get("https://flaky.com")

            assert result.status_code == 200
            assert requests.get.call_count == 2

    def test_request_timeout(self):
        """æµ‹è¯•è¯·æ±‚è¶…æ—¶"""
        client = HttpClient(timeout=1)

        with patch('requests.get', side_effect=pytest.raises(TimeoutError)):
            with pytest.raises(HttpError):
                client.get("https://slow-server.com")

    def test_request_with_headers(self):
        """æµ‹è¯•å¸¦è‡ªå®šä¹‰å¤´çš„è¯·æ±‚"""
        client = HttpClient()
        custom_headers = {
            "User-Agent": "TestAgent/1.0",
            "Authorization": "Bearer token123"
        }

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.text = "OK"

        with patch('requests.get', return_value=mock_response) as mock_get:
            result = client.get(
                "https://api.example.com",
                headers=custom_headers
            )

            # éªŒè¯è¯·æ±‚å¤´
            call_kwargs = mock_get.call_args[1]
            assert call_kwargs['headers'] == custom_headers

            assert result.status_code == 200


class TestHttpClientWithPatchObject:
    """ä½¿ç”¨ patch.object è¿›è¡Œå±€éƒ¨ mock"""

    def test_session_reuse(self):
        """æµ‹è¯•ä¼šè¯é‡ç”¨"""
        client = HttpClient()

        mock_session = MagicMock()
        mock_session.get.return_value = Mock(status_code=200, text="OK")

        with patch.object(client, 'session', mock_session):
            client.get("https://example.com")
            client.get("https://example.org")

            # éªŒè¯ä½¿ç”¨åŒä¸€ä¸ª session
            assert mock_session.get.call_count == 2

    def test_session_close_on_cleanup(self):
        """æµ‹è¯•æ¸…ç†æ—¶å…³é—­ä¼šè¯"""
        client = HttpClient()

        mock_session = MagicMock()

        with patch.object(client, 'session', mock_session):
            client.close()

            mock_session.close.assert_called_once()
```

### 7.4 Fixture ç¤ºä¾‹

```python
"""
conftest.py
å…¨å±€ fixtures é…ç½®
"""
import pytest
import asyncio
from pathlib import Path
from packages.crawler import WebCrawler, AsyncCrawler
from packages.extractors import CSSExtractor, XPathExtractor


# ==================== çˆ¬è™« Fixtures ====================

@pytest.fixture
def crawler():
    """åŒæ­¥çˆ¬è™«å®ä¾‹"""
    crawler = WebCrawler()
    yield crawler
    crawler.close()


@pytest.fixture
async def async_crawler():
    """å¼‚æ­¥çˆ¬è™«å®ä¾‹"""
    crawler = AsyncCrawler()
    yield crawler
    await crawler.close()


# ==================== æå–å™¨ Fixtures ====================

@pytest.fixture
def css_extractor():
    """CSS æå–å™¨"""
    return CSSExtractor(
        title="h1",
        content="article p",
        metadata=".metadata"
    )


@pytest.fixture
def xpath_extractor():
    """XPath æå–å™¨"""
    return XPathExtractor(
        title="//h1",
        content="//article//p",
        metadata="//div[@class='metadata']"
    )


# ==================== æµ‹è¯•æ•°æ® Fixtures ====================

@pytest.fixture
def test_html_dir():
    """æµ‹è¯• HTML æ–‡ä»¶ç›®å½•"""
    return Path(__file__).parent / "data" / "html"


@pytest.fixture
def sample_html_file(test_html_dir):
    """æä¾›ç¤ºä¾‹ HTML æ–‡ä»¶è·¯å¾„"""
    return test_html_dir / "sample.html"


@pytest.fixture
def simple_html():
    """ç®€å•çš„ HTML å†…å®¹"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Test Page</title>
    </head>
    <body>
        <h1>Main Title</h1>
        <article>
            <p>First paragraph</p>
            <p>Second paragraph</p>
        </article>
    </body>
    </html>
    """


@pytest.fixture
def complex_html():
    """å¤æ‚çš„ HTML å†…å®¹"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>Complex Page</title>
    </head>
    <body>
        <header>
            <nav>
                <ul>
                    <li><a href="/home">Home</a></li>
                    <li><a href="/about">About</a></li>
                </ul>
            </nav>
        </header>
        <main>
            <article class="post">
                <h1>Article Title</h1>
                <div class="metadata">
                    <span class="author">John Doe</span>
                    <time datetime="2024-12-25">2024-12-25</time>
                </div>
                <div class="content">
                    <p>Paragraph 1</p>
                    <p>Paragraph 2</p>
                    <blockquote>Quote</blockquote>
                    <p>Paragraph 3</p>
                </div>
            </article>
        </main>
        <footer>
            <p>Copyright 2024</p>
        </footer>
    </body>
    </html>
    """


@pytest.fixture
def various_urls():
    """å„ç§æµ‹è¯• URL"""
    return [
        "https://example.com",
        "https://example.org",
        "https://example.net",
        "http://httpbin.org/html",
    ]


# ==================== Mock Fixtures ====================

@pytest.fixture
def mock_response():
    """åˆ›å»º mock å“åº”çš„å·¥å‚å‡½æ•°"""
    def _create_response(status_code, text, json_data=None):
        mock = Mock()
        mock.status_code = status_code
        mock.text = text
        mock.ok = status_code < 400
        if json_data:
            mock.json.return_value = json_data
        return mock
    return _create_response


@pytest.fixture
def mock_http_response():
    """é¢„é…ç½®çš„ mock å“åº”"""
    mock = Mock()
    mock.status_code = 200
    mock.text = "<html><body>Test</body></html>"
    mock.ok = True
    mock.json.return_value = {"status": "success"}
    return mock


# ==================== äº‹ä»¶å¾ªç¯ Fixtures ====================

@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


# ==================== æ•°æ®åº“ Fixtures ====================

@pytest.fixture(scope="session")
def test_database():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    from packages.crawler.database import Database

    db = Database.connect(":memory:")
    yield db
    db.close()


@pytest.fixture
def clean_db(test_database):
    """æ¸…ç©ºæ•°æ®åº“çš„ fixture"""
    test_database.delete_all()
    yield test_database


# ==================== æµ‹è¯•æœåŠ¡å™¨ Fixtures ====================

@pytest.fixture(scope="session")
def test_server_url():
    """æµ‹è¯•æœåŠ¡å™¨ URLï¼ˆéœ€è¦å•ç‹¬å®ç°ï¼‰"""
    return "http://localhost:8765"


# ==================== å‚æ•°åŒ– Fixtures ====================

@pytest.fixture(params=["markdown", "html", "json"])
def output_format(request):
    """å‚æ•°åŒ–çš„è¾“å‡ºæ ¼å¼"""
    return request.param


@pytest.fixture(params=[10, 50, 100, 500])
def batch_size(request):
    """å‚æ•°åŒ–çš„æ‰¹æ¬¡å¤§å°"""
    return request.param
```

### 7.5 é›†æˆæµ‹è¯•ç¤ºä¾‹

```python
"""
test_crawler_extractor_integration.py
çˆ¬è™«ä¸æå–å™¨é›†æˆæµ‹è¯•
"""
import pytest
from packages.crawler import WebCrawler
from packages.extractors import CSSExtractor
from packages.processors import MarkdownProcessor


@pytest.mark.integration
class TestCrawlerExtractorIntegration:
    """çˆ¬è™«å’Œæå–å™¨é›†æˆæµ‹è¯•"""

    def test_crawl_and_extract(self, crawler, css_extractor, sample_html_file):
        """æµ‹è¯•çˆ¬å–å¹¶æå–æ•°æ®"""
        # ä½¿ç”¨æµ‹è¯•æœåŠ¡å™¨
        url = "http://localhost:8765/test.html"

        # çˆ¬å–ç½‘é¡µ
        crawl_result = crawler.crawl(url)

        assert crawl_result.success
        assert crawl_result.html is not None

        # æå–æ•°æ®
        extract_result = css_extractor.extract(crawl_result.html)

        assert extract_result.title is not None
        assert len(extract_result.content) > 0

    def test_full_pipeline(self, crawler, css_extractor):
        """æµ‹è¯•å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“"""
        url = "http://localhost:8765/complex.html"

        # çˆ¬å–
        crawl_result = crawler.crawl(url)
        assert crawl_result.success

        # æå–
        extract_result = css_extractor.extract(crawl_result.html)
        assert extract_result.title

        # å¤„ç†ä¸º Markdown
        processor = MarkdownProcessor()
        markdown = processor.process(
            title=extract_result.title,
            content=extract_result.content
        )

        assert f"# {extract_result.title}" in markdown
        assert len(markdown) > 0


@pytest.mark.integration
@pytest.mark.asyncio
class TestAsyncIntegration:
    """å¼‚æ­¥é›†æˆæµ‹è¯•"""

    async def test_async_crawl_and_extract(self, async_crawler, css_extractor):
        """æµ‹è¯•å¼‚æ­¥çˆ¬å–å’Œæå–"""
        url = "http://localhost:8765/async-test.html"

        # å¼‚æ­¥çˆ¬å–
        crawl_result = await async_crawler.crawl(url)
        assert crawl_result.success

        # æå–ï¼ˆåŒæ­¥æ“ä½œï¼‰
        extract_result = css_extractor.extract(crawl_result.html)
        assert extract_result.title

    async def test_batch_crawl_and_extract(self, async_crawler, css_extractor):
        """æµ‹è¯•æ‰¹é‡çˆ¬å–å’Œæå–"""
        urls = [
            "http://localhost:8765/page1.html",
            "http://localhost:8765/page2.html",
            "http://localhost:8765/page3.html",
        ]

        # æ‰¹é‡çˆ¬å–
        crawl_results = await async_crawler.crawl_batch(urls)
        assert len(crawl_results) == 3
        assert all(r.success for r in crawl_results)

        # æå–æ‰€æœ‰ç»“æœ
        extracted_data = []
        for result in crawl_results:
            extracted = css_extractor.extract(result.html)
            extracted_data.append(extracted)

        assert len(extracted_data) == 3
        assert all(d.title for d in extracted_data)
```

---

## 8. æµ‹è¯•å·¥å…·å’Œå‘½ä»¤

### 8.1 å¸¸ç”¨ pytest å‘½ä»¤

```bash
# åŸºç¡€è¿è¡Œ
pytest                              # è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/unit/                  # è¿è¡Œç‰¹å®šç›®å½•
pytest test_crawler.py              # è¿è¡Œç‰¹å®šæ–‡ä»¶
pytest test_crawler.py::TestClass   # è¿è¡Œç‰¹å®šç±»
pytest test_crawler.py::test_func   # è¿è¡Œç‰¹å®šæµ‹è¯•

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
pytest -v                           # è¯¦ç»†æ¨¡å¼
pytest -vv                          # æ›´è¯¦ç»†ï¼ˆåŒ…å« print è¾“å‡ºï¼‰
pytest -s                           # ä¸æ•è·è¾“å‡º

# è¿è¡Œå¤±è´¥çš„æµ‹è¯•
pytest --lf                         # åªè¿è¡Œä¸Šæ¬¡å¤±è´¥çš„æµ‹è¯•
pytest --ff                         # å…ˆè¿è¡Œå¤±è´¥çš„æµ‹è¯•

# è¦†ç›–ç‡
pytest --cov=packages               # æ˜¾ç¤ºè¦†ç›–ç‡
pytest --cov=packages --cov-report=html  # ç”Ÿæˆ HTML æŠ¥å‘Š
pytest --cov-fail-under=80          # è¦†ç›–ç‡ä½äº 80% åˆ™å¤±è´¥

# å¹¶è¡Œè¿è¡Œ
pytest -n auto                      # ä½¿ç”¨æ‰€æœ‰ CPU
pytest -n 4                         # ä½¿ç”¨ 4 ä¸ªè¿›ç¨‹

# æ ‡è®°
pytest -m "not slow"                # æ’é™¤æ…¢é€Ÿæµ‹è¯•
pytest -m "integration"             # åªè¿è¡Œé›†æˆæµ‹è¯•
pytest -m "network and not browser" # ç»„åˆæ ‡è®°

# è°ƒè¯•
pytest --pdb                        # å¤±è´¥æ—¶è¿›å…¥è°ƒè¯•å™¨
pytest --trace                      # æ¯ä¸ªæµ‹è¯•åè¿›å…¥è°ƒè¯•å™¨
pytest -l                           # æ˜¾ç¤ºå±€éƒ¨å˜é‡
```

### 8.2 æµ‹è¯•å¼€å‘å·¥ä½œæµ

```bash
# å¼€å‘æ–°åŠŸèƒ½æ—¶
1. ç¼–å†™æµ‹è¯•
2. è¿è¡Œæµ‹è¯•: pytest tests/unit/test_new_feature.py -v
3. ç¼–å†™å®ç°ä»£ç 
4. é‡æ–°è¿è¡Œ: pytest tests/unit/test_new_feature.py -v
5. æ£€æŸ¥è¦†ç›–ç‡: pytest --cov=packages/new_module

# æäº¤å‰æ£€æŸ¥
1. è¿è¡Œæ‰€æœ‰æµ‹è¯•: pytest
2. æ£€æŸ¥è¦†ç›–ç‡: pytest --cov=packages --cov-report=html
3. ä»£ç æ ¼å¼åŒ–: black . && isort .
4. ç±»å‹æ£€æŸ¥: mypy packages
5. Linting: flake8 packages tests
```

### 8.3 pytest æ’ä»¶æ¨è

```bash
# æ ¸å¿ƒæ’ä»¶
pytest-asyncio          # å¼‚æ­¥æµ‹è¯•æ”¯æŒ
pytest-cov              # è¦†ç›–ç‡
pytest-xdist            # å¹¶è¡Œæµ‹è¯•
pytest-mock             # Mock æ”¯æŒ
pytest-html             # HTML æŠ¥å‘Š
pytest-timeout          # è¶…æ—¶æ§åˆ¶

# é«˜çº§æ’ä»¶
pytest-benchmark        # æ€§èƒ½æµ‹è¯•
pytest-randomly         # éšæœºåŒ–æµ‹è¯•é¡ºåº
pytest-rerunfailures    # å¤±è´¥é‡è¯•
pytest-env              # ç¯å¢ƒå˜é‡ç®¡ç†
pytest-django           # Django é›†æˆ
pytest-asyncio          # å¼‚æ­¥æµ‹è¯•
```

### 8.4 ä»£ç æ¨¡æ¿

#### æµ‹è¯•æ–‡ä»¶æ¨¡æ¿

```python
"""
test_<module_name>.py
<ç®€çŸ­æè¿°>
"""
import pytest
from packages.<module_name> import <ClassName>


class Test<ClassName>:
    """æµ‹è¯• <ClassName>"""

    @pytest.fixture
    def instance(self):
        """æä¾›æµ‹è¯•å®ä¾‹"""
        return <ClassName>()

    def test_<scenario>_when_<condition>_then_<expected_result>(self, instance):
        """æµ‹è¯•åœºæ™¯æè¿°"""
        # Given
        input_data = ...

        # When
        result = instance.method(input_data)

        # Then
        assert result == expected_output
```

#### conftest.py æ¨¡æ¿

```python
"""
conftest.py
å…¨å±€æµ‹è¯•é…ç½®å’Œ fixtures
"""
import pytest
from packages.crawler import WebCrawler


@pytest.fixture(scope="session")
def global_config():
    """å…¨å±€é…ç½®"""
    return {
        "timeout": 30,
        "max_retries": 3,
    }


def pytest_configure(config):
    """pytest é…ç½®é’©å­"""
    config.addinivalue_line("markers", "slow: æ ‡è®°æ…¢é€Ÿæµ‹è¯•")
    config.addinivalue_line("markers", "integration: é›†æˆæµ‹è¯•")
    config.addinivalue_line("markers", "network: éœ€è¦ç½‘ç»œ")


@pytest.fixture(autouse=True)
def reset_state():
    """æ¯ä¸ªæµ‹è¯•å‰é‡ç½®çŠ¶æ€"""
    yield
    # æ¸…ç†ä»£ç 
```

---

## é™„å½•

### A. æµ‹è¯•æ£€æŸ¥æ¸…å•

åœ¨æäº¤ä»£ç å‰ï¼Œç¡®ä¿ï¼š

- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ˆ`pytest`ï¼‰
- [ ] è¦†ç›–ç‡è¾¾åˆ°è¦æ±‚ï¼ˆ`pytest --cov`ï¼‰
- [ ] ä»£ç é€šè¿‡æ ¼å¼æ£€æŸ¥ï¼ˆ`black`, `isort`ï¼‰
- [ ] ç±»å‹æ£€æŸ¥é€šè¿‡ï¼ˆ`mypy`ï¼‰
- [ ] Linting é€šè¿‡ï¼ˆ`flake8`ï¼‰
- [ ] æ·»åŠ äº†å¿…è¦çš„æµ‹è¯•æ–‡æ¡£
- [ ] æµ‹è¯•å‘½åæ¸…æ™°ã€å…·æœ‰æè¿°æ€§
- [ ] å¼‚æ­¥ä»£ç ä½¿ç”¨äº† `@pytest.mark.asyncio`
- [ ] Mock ä½¿ç”¨æ­£ç¡®
- [ ] Fixture ä½œç”¨åŸŸåˆç†

### B. å¸¸è§é—®é¢˜

**Q: å¦‚ä½•æµ‹è¯•ç§æœ‰æ–¹æ³•ï¼Ÿ**
A: é€šè¿‡å…¬å…±æ¥å£æµ‹è¯•ï¼Œä¸åº”ç›´æ¥æµ‹è¯•ç§æœ‰æ–¹æ³•ã€‚å¦‚æœå¿…é¡»æµ‹è¯•ï¼Œè€ƒè™‘å°†å…¶è®¾ä¸º protected æˆ–é‡æ„ä»£ç ã€‚

**Q: Mock å¤ªå¤šæ€ä¹ˆåŠï¼Ÿ**
A: å¦‚æœ Mock å¤ªå¤šï¼Œå¯èƒ½è¯´æ˜ä»£ç è®¾è®¡æœ‰é—®é¢˜ã€‚è€ƒè™‘ï¼š
- æå–æ¥å£
- ä½¿ç”¨ä¾èµ–æ³¨å…¥
- æ”¹è¿›ä»£ç ç»“æ„

**Q: å¦‚ä½•æµ‹è¯•éšæœºæ€§ï¼Ÿ**
A: ä½¿ç”¨å›ºå®šçš„ç§å­ï¼š
```python
import random
random.seed(42)
```

**Q: å¼‚æ­¥æµ‹è¯•å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**
A: ä½¿ç”¨ Mock æˆ– fixture é¿å…çœŸå® I/Oï¼š
```python
@pytest.mark.asyncio
async def test_with_mock():
    with patch('asyncio.sleep', return_value=AsyncMock()):
        # æµ‹è¯•ä»£ç 
```

### C. å‚è€ƒèµ„æ–™

- [pytest å®˜æ–¹æ–‡æ¡£](https://docs.pytest.org/)
- [pytest-asyncio æ–‡æ¡£](https://pytest-asyncio.readthedocs.io/)
- [unittest.mock æ–‡æ¡£](https://docs.python.org/3/library/unittest.mock.html)
- [Python æµ‹è¯•æœ€ä½³å®è·µ](https://docs.python-guide.org/writing/tests/)

---

**æœ€åæ›´æ–°**ï¼š2024-12-25
**ç»´æŠ¤è€…**ï¼šAwesome-crawl4AI Team
**åé¦ˆ**ï¼šè¯·åœ¨ GitHub Issues æå‡ºé—®é¢˜å’Œå»ºè®®
