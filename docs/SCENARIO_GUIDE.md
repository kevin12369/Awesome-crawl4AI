# Awesome-crawl4AI åœºæ™¯å¼€å‘æŒ‡å—

è‰¹ï¼Œè¿™æ˜¯å®Œæ•´çš„åœºæ™¯å¼€å‘æ•™ç¨‹ï¼Œè·Ÿç€Kevinä¸€æ­¥æ­¥æ¥ï¼

---

## ç›®å½• / Table of Contents

- [ä»€ä¹ˆæ˜¯åœºæ™¯ / What is a Scenario](#ä»€ä¹ˆæ˜¯åœºæ™¯)
- [ä¸¤ç§åˆ›å»ºæ–¹å¼ / Two Creation Methods](#ä¸¤ç§åˆ›å»ºæ–¹å¼)
- [æ–¹å¼ä¸€ï¼šå¯è§†åŒ–ç¼–è¾‘å™¨ / Method 1: Visual Editor](#æ–¹å¼ä¸€å¯è§†åŒ–ç¼–è¾‘å™¨)
- [æ–¹å¼äºŒï¼šä»£ç å¼€å‘ / Method 2: Code Development](#æ–¹å¼äºŒä»£ç å¼€å‘)
- [åœºæ™¯ç¤ºä¾‹ / Scenario Examples](#åœºæ™¯ç¤ºä¾‹)
- [æœ€ä½³å®è·µ / Best Practices](#æœ€ä½³å®è·µ)

---

## ä»€ä¹ˆæ˜¯åœºæ™¯ / What is a Scenario

**åœºæ™¯ï¼ˆScenarioï¼‰** æ˜¯é’ˆå¯¹ç‰¹å®šç½‘ç«™ç±»å‹æˆ–æ•°æ®æå–éœ€æ±‚è®¾è®¡çš„é¢„é…ç½®æ¨¡æ¿ã€‚

**A Scenario** is a pre-configured template designed for specific website types or data extraction needs.

### å†…ç½®åœºæ™¯ / Built-in Scenarios

| åœºæ™¯åç§° | åˆ†ç±» | ç”¨é€” |
|---------|------|------|
| `NewsCrawler` | news | æ–°é—»æ–‡ç« çˆ¬å– |
| `DocsArchiver` | docs | æŠ€æœ¯æ–‡æ¡£å½’æ¡£ |
| `EcommerceMonitor` | ecommerce | ç”µå•†ä»·æ ¼ç›‘æ§ |
| `AcademicCollector` | academic | å­¦æœ¯è®ºæ–‡æ”¶é›† |
| `TableExtractor` | table | è¡¨æ ¼æ•°æ®æå– |

---

## ä¸¤ç§åˆ›å»ºæ–¹å¼ / Two Creation Methods

### ğŸ¨ æ–¹å¼ä¸€ï¼šå¯è§†åŒ–ç¼–è¾‘å™¨ / Method 1: Visual Editorï¼ˆæ¨èï¼‰

**é€‚åˆï¼š** å¿«é€Ÿåˆ›å»ºç®€å•åœºæ™¯ï¼Œæ— éœ€ç¼–ç¨‹
**Suitable for:** Quick creation of simple scenarios, no coding required

### ğŸ’» æ–¹å¼äºŒï¼šä»£ç å¼€å‘ / Method 2: Code Development

**é€‚åˆï¼š** å¤æ‚åœºæ™¯ï¼Œéœ€è¦è‡ªå®šä¹‰é€»è¾‘
**Suitable for:** Complex scenarios requiring custom logic

---

## æ–¹å¼ä¸€ï¼šå¯è§†åŒ–ç¼–è¾‘å™¨ / Method 1: Visual Editor

### æ­¥éª¤ 1ï¼šæ‰“å¼€æ¨¡æ¿ç¼–è¾‘å™¨

1. è®¿é—® `http://localhost:8000`
2. ç‚¹å‡»å·¦ä¾§èœå• **ã€Œåœºæ™¯æ¨¡æ¿ã€**
3. ç‚¹å‡»å³ä¸Šè§’ **ã€Œ+ æ–°å»ºæ¨¡æ¿ã€**

### æ­¥éª¤ 2ï¼šå¡«å†™åŸºæœ¬ä¿¡æ¯

**å¿…å¡«å­—æ®µ / Required Fieldsï¼š**

| å­—æ®µ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| æ¨¡æ¿åç§° | å”¯ä¸€æ ‡è¯†ç¬¦ | `blog_crawler` |
| æè¿° | æ¨¡æ¿ç”¨é€”è¯´æ˜ | `åšå®¢æ–‡ç« çˆ¬å– - æå–æ ‡é¢˜ã€å†…å®¹ã€æ ‡ç­¾` |
| åˆ†ç±» | é€‰æ‹©åˆ†ç±» | `custom` |

### æ­¥éª¤ 3ï¼šæ·»åŠ æå–å­—æ®µ

ç‚¹å‡» **ã€Œ+ æ·»åŠ å­—æ®µã€**ï¼Œå¡«å†™æ¯ä¸ªå­—æ®µçš„é…ç½®ï¼š

**å­—æ®µé…ç½®ç¤ºä¾‹ / Field Configuration Exampleï¼š**

```
å­—æ®µ1ï¼šæ ‡é¢˜
- åç§°ï¼štitle
- CSSé€‰æ‹©å™¨ï¼šh1, .post-title, [itemprop='headline']
- ç±»å‹ï¼štext
- å¿…å¡«ï¼šâœ…

å­—æ®µ2ï¼šå†…å®¹
- åç§°ï¼šcontent
- CSSé€‰æ‹©å™¨ï¼šarticle .content, .post-body
- ç±»å‹ï¼štext
- å¿…å¡«ï¼šâœ…

å­—æ®µ3ï¼šæ ‡ç­¾
- åç§°ï¼štags
- CSSé€‰æ‹©å™¨ï¼š.tags a, .post-tags li
- ç±»å‹ï¼štext
- å¤šå€¼ï¼šâœ…
```

### æ­¥éª¤ 4ï¼šé«˜çº§é…ç½®ï¼ˆå¯é€‰ï¼‰

å±•å¼€ **ã€Œé«˜çº§é…ç½®ã€** è®¾ç½®ï¼š

| é…ç½®é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| è¯·æ±‚å»¶è¿Ÿ | 0.5ç§’ | ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´çš„é—´éš”ï¼ˆç¤¼è²Œçˆ¬å–ï¼‰ |
| æ·±åº¦çˆ¬å– | false | æ˜¯å¦çˆ¬å–é“¾æ¥é¡µé¢ |
| æœ€å¤§é¡µé¢æ•° | 50 | æ·±åº¦çˆ¬å–æ—¶çš„é¡µé¢é™åˆ¶ |
| çˆ¬å–ç­–ç•¥ | bfs | BFSï¼ˆå¹¿åº¦ä¼˜å…ˆï¼‰æˆ– DFSï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰ |
| æ»šåŠ¨åŠ è½½ | false | æ˜¯å¦æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹ |
| æœ€å¤§æ»šåŠ¨æ¬¡æ•° | 5 | æ»šåŠ¨æ¬¡æ•°é™åˆ¶ |

### æ­¥éª¤ 5ï¼šéªŒè¯å¹¶ä¿å­˜

1. ç‚¹å‡» **ã€ŒéªŒè¯ã€** æŒ‰é’®æ£€æŸ¥é…ç½®
2. éªŒè¯é€šè¿‡åç‚¹å‡» **ã€Œä¿å­˜ã€**

### ä½¿ç”¨æ–°åˆ›å»ºçš„æ¨¡æ¿

åœ¨ **Dashboard** æˆ– **åœºæ™¯æ¨¡æ¿** é¡µé¢ï¼š
1. æ‰¾åˆ°ä½ åˆ›å»ºçš„æ¨¡æ¿
2. ç‚¹å‡» **ã€Œåº”ç”¨ã€**
3. è¾“å…¥ç›®æ ‡URL
4. å¼€å§‹çˆ¬å–ï¼

---

## æ–¹å¼äºŒï¼šä»£ç å¼€å‘ / Method 2: Code Development

### å®Œæ•´ä»£ç ç¤ºä¾‹ / Complete Code Example

```python
"""
åšå®¢çˆ¬å–åœºæ™¯
Blog Crawler Scenario

ä¸“é—¨ç”¨äºä¸ªäººåšå®¢æ–‡ç« çš„æå–
Specialized for personal blog post extraction
"""

from typing import Any
from .base import BaseScenario
from ..core.template_engine import TemplateConfigSchema, ExtractField, AdvancedConfig


class BlogCrawler(BaseScenario):
    """
    åšå®¢çˆ¬å–åœºæ™¯
    Blog Crawler Scenario

    è‰¹ï¼Œæå–åšå®¢æ–‡ç« çš„æ ‡é¢˜ã€å†…å®¹ã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸå’Œæ ‡ç­¾ï¼
    """

    def get_schema(self) -> TemplateConfigSchema:
        """è·å–åšå®¢çˆ¬å–é…ç½®Schema"""
        return TemplateConfigSchema(
            name="blog_crawler",
            description="åšå®¢çˆ¬å– - æå–åšå®¢æ–‡ç« çš„æ ‡é¢˜ã€å†…å®¹ã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸå’Œæ ‡ç­¾",
            category="custom",  # è‡ªå®šä¹‰åˆ†ç±»
            fields=[
                # è‰¹ï¼Œæ ‡é¢˜å­—æ®µ - æ”¯æŒå¤šç§é€‰æ‹©å™¨
                ExtractField(
                    name="title",
                    selector="h1, .post-title, .entry-title, [itemprop*='headline']",
                    type="text",
                    required=True,
                ),
                # è‰¹ï¼Œå†…å®¹å­—æ®µ - ä¼˜å…ˆarticleæ ‡ç­¾
                ExtractField(
                    name="content",
                    selector="article .content, .post-body, .entry-content",
                    type="text",
                    required=True,
                ),
                # è‰¹ï¼Œä½œè€…å­—æ®µ
                ExtractField(
                    name="author",
                    selector=".author, .post-author, [itemprop*='author']",
                    type="text",
                ),
                # è‰¹ï¼Œå‘å¸ƒæ—¥æœŸ
                ExtractField(
                    name="publish_date",
                    selector="time, .date, .publish-date, [itemprop*='datePublished']",
                    type="text",
                ),
                # è‰¹ï¼Œæ ‡ç­¾ - å¤šå€¼å­—æ®µ
                ExtractField(
                    name="tags",
                    selector=".tags a, .post-tags li, [rel*='tag']",
                    type="text",
                    multiple=True,
                ),
            ],
            advanced=AdvancedConfig(
                delay=1.0,  # è‰¹ï¼Œåšå®¢ç½‘ç«™å‹å¥½ï¼Œå»¶è¿Ÿ1ç§’
                deep_crawl=False,  # ä¸éœ€è¦æ·±åº¦çˆ¬å–
            ),
        )

    async def extract(self, url: str, crawler) -> dict[str, Any]:
        """
        æ‰§è¡Œåšå®¢çˆ¬å–

        Args:
            url: åšå®¢æ–‡ç« URL
            crawler: Crawl4AIå°è£…å®ä¾‹

        Returns:
            dict: æå–ç»“æœ
        """
        # è‰¹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        result = await crawler.crawl(url)

        if not result.get("success"):
            return result

        # TODO: åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šæ¸…ç†Markdownæ ¼å¼ã€æå–å›¾ç‰‡ç­‰

        return result


# è‰¹ï¼Œè‡ªåŠ¨æ³¨å†Œåˆ°åœºæ™¯æ³¨å†Œè¡¨ï¼
from ..core.scenario_registry import register_scenario
register_scenario(BlogCrawler)
```

### ä»£ç ç»“æ„è¯´æ˜ / Code Structure Explanation

#### 1. å¯¼å…¥ä¾èµ– / Import Dependencies

```python
from typing import Any
from .base import BaseScenario  # è‰¹ï¼Œæ‰€æœ‰åœºæ™¯å¿…é¡»ç»§æ‰¿BaseScenario
from ..core.template_engine import TemplateConfigSchema, ExtractField, AdvancedConfig
```

#### 2. å®šä¹‰åœºæ™¯ç±» / Define Scenario Class

```python
class BlogCrawler(BaseScenario):
    """ç±»åè¦æ¸…æ™°è¡¨è¾¾ç”¨é€” / Class name should clearly express purpose"""
```

#### 3. å®ç° get_schema() / Implement get_schema()

```python
def get_schema(self) -> TemplateConfigSchema:
    """è¿”å›åœºæ™¯é…ç½®Schema / Return scenario configuration schema"""
    return TemplateConfigSchema(
        name="blog_crawler",          # å”¯ä¸€åç§° / Unique name
        description="åšå®¢çˆ¬å–",        # æè¿° / Description
        category="custom",            # åˆ†ç±» / Category
        fields=[...],                 # æå–å­—æ®µ / Extract fields
        advanced=AdvancedConfig(...)  # é«˜çº§é…ç½® / Advanced config
    )
```

#### 4. å®ç° extract() / Implement extract()

```python
async def extract(self, url: str, crawler) -> dict[str, Any]:
    """
    æ‰§è¡Œçˆ¬å–çš„å¼‚æ­¥æ–¹æ³•
    Async method to execute crawling

    Args:
        url: ç›®æ ‡URL / Target URL
        crawler: Crawl4AIWrapperå®ä¾‹ / Crawl4AIWrapper instance

    Returns:
        dict: æå–ç»“æœ / Extraction result
    """
    result = await crawler.crawl(url)
    return result
```

#### 5. è‡ªåŠ¨æ³¨å†Œ / Auto Registration

```python
from ..core.scenario_registry import register_scenario
register_scenario(BlogCrawler)
```

### æ”¾ç½®æ–‡ä»¶ä½ç½® / File Placement

å°†æ–‡ä»¶ä¿å­˜åˆ°ï¼š`backend/scenarios/blog_crawler.py`

ç„¶ååœ¨ `backend/scenarios/__init__.py` ä¸­å¯¼å…¥ï¼š

```python
from .blog_crawler import BlogCrawler

__all__ = [
    # ... å…¶ä»–åœºæ™¯
    "BlogCrawler",
]
```

### é‡å¯æœåŠ¡ç”Ÿæ•ˆ / Restart to Take Effect

```bash
# åœæ­¢æœåŠ¡ / Stop service
# Ctrl+C

# é‡æ–°å¯åŠ¨ / Restart
cd backend
python -m uvicorn main:app --reload
```

---

## åœºæ™¯ç¤ºä¾‹ / Scenario Examples

### ç¤ºä¾‹1ï¼šç¤¾äº¤åª’ä½“å¸–å­ / Social Media Post

```python
class SocialMediaPost(BaseScenario):
    def get_schema(self) -> TemplateConfigSchema:
        return TemplateConfigSchema(
            name="social_media_post",
            description="ç¤¾äº¤åª’ä½“å¸–å­æå–",
            category="custom",
            fields=[
                ExtractField(
                    name="username",
                    selector=".username, .user-name, [data-user]",
                    type="text",
                    required=True,
                ),
                ExtractField(
                    name="post_content",
                    selector=".post-content, .tweet-text, .message",
                    type="text",
                    required=True,
                ),
                ExtractField(
                    name="likes",
                    selector=".likes-count, .like-count",
                    type="number",
                ),
                ExtractField(
                    name="comments",
                    selector=".comment .text",
                    type="text",
                    multiple=True,
                ),
                ExtractField(
                    name="images",
                    selector=".post-image img",
                    type="image",
                    multiple=True,
                ),
            ],
            advanced=AdvancedConfig(
                delay=2.0,  # è‰¹ï¼Œç¤¾äº¤åª’ä½“åçˆ¬ä¸¥æ ¼
                scroll_to_load=True,
                max_scrolls=3,
            ),
        )
```

### ç¤ºä¾‹2ï¼šæˆ¿äº§ä¿¡æ¯ / Real Estate Listing

```python
class RealEstateListing(BaseScenario):
    def get_schema(self) -> TemplateConfigSchema:
        return TemplateConfigSchema(
            name="real_estate_listing",
            description="æˆ¿äº§ä¿¡æ¯æå–",
            category="custom",
            fields=[
                ExtractField(
                    name="property_title",
                    selector="h1, .listing-title",
                    type="text",
                    required=True,
                ),
                ExtractField(
                    name="price",
                    selector=".price, .listing-price",
                    type="text",
                    required=True,
                ),
                ExtractField(
                    name="address",
                    selector=".address, .property-address",
                    type="text",
                ),
                ExtractField(
                    name="bedrooms",
                    selector=".bedrooms, .bd",
                    type="number",
                ),
                ExtractField(
                    name="bathrooms",
                    selector=".bathrooms, .ba",
                    type="number",
                ),
                ExtractField(
                    name="area",
                    selector=".area, .sqft",
                    type="number",
                ),
                ExtractField(
                    name="description",
                    selector=".description, .listing-desc",
                    type="text",
                ),
                ExtractField(
                    name="images",
                    selector=".gallery img, .property-image img",
                    type="image",
                    multiple=True,
                ),
            ],
        )
```

### ç¤ºä¾‹3ï¼šæ‹›è˜ä¿¡æ¯ / Job Posting

```python
class JobPosting(BaseScenario):
    def get_schema(self) -> TemplateConfigSchema:
        return TemplateConfigSchema(
            name="job_posting",
            description="æ‹›è˜ä¿¡æ¯æå–",
            category="custom",
            fields=[
                ExtractField(
                    name="job_title",
                    selector="h1, .job-title",
                    type="text",
                    required=True,
                ),
                ExtractField(
                    name="company",
                    selector=".company-name, [itemprop='hiringOrganization']",
                    type="text",
                ),
                ExtractField(
                    name="location",
                    selector=".location, [itemprop='jobLocation']",
                    type="text",
                ),
                ExtractField(
                    name="salary",
                    selector=".salary, .compensation",
                    type="text",
                ),
                ExtractField(
                    name="description",
                    selector=".job-description, .description",
                    type="text",
                ),
                ExtractField(
                    name="requirements",
                    selector=".requirements li",
                    type="text",
                    multiple=True,
                ),
                ExtractField(
                    name="benefits",
                    selector=".benefits li",
                    type="text",
                    multiple=True,
                ),
            ],
        )
```

---

## é«˜çº§ç”¨æ³• / Advanced Usage

### è‡ªå®šä¹‰æ•°æ®å¤„ç† / Custom Data Processing

```python
async def extract(self, url: str, crawler) -> dict[str, Any]:
    """æ‰§è¡Œè‡ªå®šä¹‰æ•°æ®å¤„ç†"""
    result = await crawler.crawl(url)

    if not result.get("success"):
        return result

    # è‰¹ï¼Œè‡ªå®šä¹‰å¤„ç†ï¼šæ¸…ç†Markdown
    markdown = result.get("markdown", "")
    cleaned_markdown = self._clean_markdown(markdown)
    result["markdown"] = cleaned_markdown

    # è‰¹ï¼Œæå–æ‰€æœ‰å›¾ç‰‡é“¾æ¥
    images = result.get("media", {}).get("images", [])
    image_urls = [img.get("src") for img in images if img.get("src")]
    result["extracted_content"]["image_urls"] = image_urls

    return result

def _clean_markdown(self, markdown: str) -> str:
    """æ¸…ç†Markdownæ ¼å¼"""
    # ç§»é™¤å¤šä½™ç©ºè¡Œ
    lines = [line for line in markdown.split("\n") if line.strip()]
    return "\n\n".join(lines)
```

### ä½¿ç”¨æ·±åº¦çˆ¬å– / Using Deep Crawl

```python
async def extract(self, url: str, crawler) -> dict[str, Any]:
    """æ·±åº¦çˆ¬å–æ•´ä¸ªæ–‡æ¡£ç½‘ç«™"""
    result = await crawler.deep_crawl(
        url,
        strategy="bfs",  # BFSæˆ–DFS
        max_pages=100,   # æœ€å¤š100é¡µ
        config={"bypass_cache": True},
    )
    return result
```

### æ·»åŠ é‡è¯•é€»è¾‘ / Add Retry Logic

```python
async def extract(self, url: str, crawler) -> dict[str, Any]:
    """å¸¦é‡è¯•çš„çˆ¬å–"""
    max_retries = 3
    for attempt in range(max_retries):
        result = await crawler.crawl(url)

        if result.get("success"):
            return result

        # è‰¹ï¼Œæœ€åä¸€æ¬¡å°è¯•å¤±è´¥
        if attempt == max_retries - 1:
            result["error_message"] = f"é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥"
            return result

        # ç­‰å¾…åé‡è¯•
        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    return {"success": False, "error_message": "æœªçŸ¥é”™è¯¯"}
```

---

## æœ€ä½³å®è·µ / Best Practices

### âœ… DO - åº”è¯¥åšçš„

1. **ä½¿ç”¨å¤šä¸ªCSSé€‰æ‹©å™¨** - æé«˜å…¼å®¹æ€§
   ```python
   selector="h1, .title, [itemprop='headline']"
   ```

2. **è®¾ç½®åˆç†çš„å»¶è¿Ÿ** - ç¤¼è²Œçˆ¬å–
   ```python
   advanced=AdvancedConfig(delay=1.0)  # è‡³å°‘1ç§’
   ```

3. **æ·»åŠ è¯¦ç»†æ³¨é‡Š** - æ–¹ä¾¿åç»­ç»´æŠ¤
   ```python
   # è‰¹ï¼Œæ ‡é¢˜å­—æ®µ - æ”¯æŒå¤šç§å¸¸è§HTMLç»“æ„
   ExtractField(name="title", selector="h1, .post-title")
   ```

4. **ä½¿ç”¨requiredæ ‡è®°** - ç¡®ä¿å…³é”®æ•°æ®
   ```python
   ExtractField(name="title", required=True)
   ```

5. **æµ‹è¯•å¤šä¸ªç½‘ç«™** - ç¡®ä¿é€šç”¨æ€§

### âŒ DON'T - ä¸åº”è¯¥åšçš„

1. **ä¸è¦è®¾ç½®å¤ªçŸ­çš„å»¶è¿Ÿ** - ä¼šè¢«å°IP
   ```python
   # âŒ é”™è¯¯ / Wrong
   advanced=AdvancedConfig(delay=0.1)

   # âœ… æ­£ç¡® / Correct
   advanced=AdvancedConfig(delay=1.0)
   ```

2. **ä¸è¦è¿‡åº¦çˆ¬å–** - éµå®ˆrobots.txt
   ```python
   # âŒ é”™è¯¯ / Wrong
   max_pages=10000

   # âœ… æ­£ç¡® / Correct
   max_pages=100
   ```

3. **ä¸è¦å¿½ç•¥é”™è¯¯å¤„ç†**
   ```python
   # âŒ é”™è¯¯ / Wrong
   result = await crawler.crawl(url)
   return result

   # âœ… æ­£ç¡® / Correct
   result = await crawler.crawl(url)
   if not result.get("success"):
       return {"success": False, "error": result.get("error")}
   return result
   ```

4. **ä¸è¦ä½¿ç”¨è¿‡äºå…·ä½“çš„é€‰æ‹©å™¨**
   ```python
   # âŒ é”™è¯¯ / Wrong
   selector="#post-12345 .title"  # å¤ªå…·ä½“

   # âœ… æ­£ç¡® / Correct
   selector=".post-title, h1"  # é€šç”¨
   ```

---

## CSSé€‰æ‹©å™¨æŠ€å·§ / CSS Selector Tips

### åŸºæœ¬é€‰æ‹©å™¨ / Basic Selectors

```css
/* å…ƒç´ é€‰æ‹©å™¨ */
h1, p, div

/* ç±»é€‰æ‹©å™¨ */
.title, .content

/* IDé€‰æ‹©å™¨ */
#main-title

/* å±æ€§é€‰æ‹©å™¨ */
[itemprop='headline']
[data-id]
```

### ç»„åˆé€‰æ‹©å™¨ / Combinators

```css
/* åä»£é€‰æ‹©å™¨ */
article .content

/* å­é€‰æ‹©å™¨ */
article > .content

/* å¤šä¸ªé€‰æ‹©å™¨ï¼ˆæˆ–ï¼‰ */
h1, .title, [itemprop='headline']
```

### ä¼ªç±» / Pseudo-classes

```css
/* ç¬¬ä¸€ä¸ªå­å…ƒç´  */
ul li:first-child

/* æœ€åä¸€ä¸ªå­å…ƒç´  */
ul li:last-child

/* ç¬¬Nä¸ªå­å…ƒç´  */
ul li:nth-child(2)
```

---

## è°ƒè¯•æŠ€å·§ / Debugging Tips

### 1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·

1. æ‰“å¼€ç›®æ ‡ç½‘é¡µ
2. æŒ‰ `F12` æ‰“å¼€å¼€å‘è€…å·¥å…·
3. ç‚¹å‡»å…ƒç´ é€‰æ‹©å™¨ï¼ˆå·¦ä¸Šè§’ç®­å¤´å›¾æ ‡ï¼‰
4. ç‚¹å‡»é¡µé¢å…ƒç´ æŸ¥çœ‹å…¶HTMLç»“æ„
5. å³é”® â†’ Copy â†’ Copy selector

### 2. æµ‹è¯•CSSé€‰æ‹©å™¨

åœ¨æµè§ˆå™¨æ§åˆ¶å°è¿è¡Œï¼š
```javascript
// æµ‹è¯•å•ä¸ªé€‰æ‹©å™¨
document.querySelector("h1")

// æµ‹è¯•å¤šä¸ªé€‰æ‹©å™¨
document.querySelectorAll(".tags a")

// æŸ¥çœ‹å…ƒç´ å†…å®¹
document.querySelector("h1").textContent
```

### 3. æŸ¥çœ‹çˆ¬å–ç»“æœ

```python
# åœ¨extractæ–¹æ³•ä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
import json

async def extract(self, url: str, crawler) -> dict[str, Any]:
    result = await crawler.crawl(url)

    # è‰¹ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
    print("=" * 50)
    print(f"URL: {url}")
    print(f"Success: {result.get('success')}")
    print(f"Extracted: {json.dumps(result.get('extracted_content'), indent=2)}")
    print("=" * 50)

    return result
```

---

## å¸¸è§é—®é¢˜ / FAQ

### Q1: ä¸ºä»€ä¹ˆæå–ä¸åˆ°æ•°æ®ï¼Ÿ

**A:** å¯èƒ½çš„åŸå› ï¼š
1. CSSé€‰æ‹©å™¨ä¸æ­£ç¡® â†’ ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·éªŒè¯
2. é¡µé¢æ˜¯JavaScriptåŠ¨æ€æ¸²æŸ“ â†’ å¯ç”¨ `scroll_to_load`
3. ç½‘ç«™æœ‰åçˆ¬æœºåˆ¶ â†’ å¢åŠ  `delay` å»¶è¿Ÿ

### Q2: å¦‚ä½•å¤„ç†JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼Ÿ

**A:** Crawl4AIä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†å¯èƒ½éœ€è¦æ»šåŠ¨åŠ è½½ï¼š
```python
advanced=AdvancedConfig(
    scroll_to_load=True,
    max_scrolls=5,
)
```

### Q3: å¦‚ä½•æå–å›¾ç‰‡çš„altå±æ€§ï¼Ÿ

**A:** ä½¿ç”¨ `attribute` ç±»å‹å¹¶æŒ‡å®šå±æ€§åï¼š
```python
ExtractField(
    name="image_alt",
    selector=".main-image",
    type="attribute",
    attribute="alt",
)
```

### Q4: å¯ä»¥çˆ¬å–éœ€è¦ç™»å½•çš„ç½‘ç«™å—ï¼Ÿ

**A:** ç›®å‰ç‰ˆæœ¬ä¸æ”¯æŒï¼Œéœ€è¦ä½¿ç”¨Crawl4AIçš„SessionåŠŸèƒ½ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

### Q5: å¦‚ä½•é¿å…è¢«å°IPï¼Ÿ

**A:**
1. è®¾ç½®åˆç†çš„å»¶è¿Ÿï¼ˆè‡³å°‘1ç§’ï¼‰
2. ä½¿ç”¨ä»£ç†æ± ï¼ˆåœ¨AdvancedConfigä¸­é…ç½®ï¼‰
3. éµå®ˆç½‘ç«™çš„robots.txt

---

è‰¹ï¼Œåœºæ™¯å¼€å‘æŒ‡å—å†™å®Œäº†ï¼æœ‰é—®é¢˜æ‰¾Kevinï¼

**æ–‡æ¡£ç‰ˆæœ¬ / Versionï¼š** 1.0.0
**æœ€åæ›´æ–° / Last Updatedï¼š** 2025-12-25
