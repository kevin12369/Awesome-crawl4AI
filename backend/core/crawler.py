"""
Crawl4AI å°è£…å±‚
Crawl4AI Wrapper

è¿™ä¸ªSBæ¨¡å—å°è£…Crawl4AIçš„AsyncWebCrawlerï¼Œæä¾›ç»Ÿä¸€çš„çˆ¬å–æ¥å£
This module wraps Crawl4AI's AsyncWebCrawler for unified crawl interface
"""

import asyncio
from typing import Any, Optional
from pathlib import Path

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy


class Crawl4AIWrapper:
    """
    Crawl4AIå°è£…ç±»
    Crawl4AI Wrapper Class

    è‰¹ï¼Œè¿™ä¸ªç±»ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬å–æ“ä½œï¼Œåˆ«tmåˆ°å¤„åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼
    """

    def __init__(
        self,
        headless: bool = True,
        browser_type: str = "chromium",
        verbose: bool = True,
    ):
        """
        åˆå§‹åŒ–å°è£…å™¨

        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            browser_type: æµè§ˆå™¨ç±»å‹ï¼ˆchromium/firefox/webkitï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.browser_config = BrowserConfig(
            headless=headless,
            browser_type=browser_type,
            verbose=verbose,
        )
        self.verbose = verbose
        self._crawler: Optional[AsyncWebCrawler] = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self._crawler = AsyncWebCrawler(config=self.browser_config)
        await self._crawler.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self._crawler:
            await self._crawler.__aexit__(exc_type, exc_val, exc_tb)

    async def crawl(
        self,
        url: str,
        config: Optional[dict[str, Any]] = None,
    ) -> dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªURL
        Crawl a single URL

        Args:
            url: ç›®æ ‡URL
            config: çˆ¬å–é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            dict: çˆ¬å–ç»“æœå­—å…¸
            {
                "success": bool,
                "markdown": str,
                "fit_markdown": str,
                "extracted_content": str,
                "links": dict,
                "media": dict,
                "metadata": dict,
                "screenshot": str,
                "error": str  # å¦‚æœå¤±è´¥
            }
        """
        if not self._crawler:
            raise RuntimeError("è‰¹ï¼Œçˆ¬è™«æœªåˆå§‹åŒ–ï¼è¯·ä½¿ç”¨ async with è¯­å¥ã€‚")

        try:
            # æ„å»ºçˆ¬å–é…ç½®
            run_config = self._build_run_config(config or {})

            # æ‰§è¡Œçˆ¬å–
            if self.verbose:
                print(f"ğŸ” å¼€å§‹çˆ¬å–: {url}")

            result = await self._crawler.arun(url=url, config=run_config)

            # å¤„ç†ç»“æœ
            if result.success:
                if self.verbose:
                    print(f"âœ… çˆ¬å–æˆåŠŸ: {url}")

                return {
                    "success": True,
                    "markdown": result.markdown.raw_markdown if result.markdown else "",
                    "fit_markdown": result.markdown.fit_markdown if result.markdown else "",
                    "extracted_content": result.extracted_content,
                    "links": {
                        "internal": result.links.get("internal", []),
                        "external": result.links.get("external", []),
                    } if result.links else {},
                    "media": {
                        "images": result.media.get("images", []),
                        "videos": result.media.get("videos", []),
                        "audio": result.media.get("audio", []),
                    } if result.media else {},
                    "metadata": {
                        "title": result.metadata.get("title"),
                        "description": result.metadata.get("description"),
                        "keywords": result.metadata.get("keywords", []),
                    } if result.metadata else {},
                    "screenshot": result.screenshot,
                }
            else:
                error_msg = result.error_message or "çˆ¬å–å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯"
                if self.verbose:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {url} - {error_msg}")

                return {
                    "success": False,
                    "error": error_msg,
                }

        except Exception as e:
            error_msg = f"çˆ¬å–å¼‚å¸¸: {str(e)}"
            if self.verbose:
                print(f"âŒ {error_msg}")

            return {
                "success": False,
                "error": error_msg,
            }

    async def crawl_batch(
        self,
        urls: list[str],
        config: Optional[dict[str, Any]] = None,
        max_concurrent: int = 5,
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–URL
        Batch crawl URLs

        Args:
            urls: URLåˆ—è¡¨
            config: çˆ¬å–é…ç½®ï¼ˆå¯é€‰ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            list: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        if not self._crawler:
            raise RuntimeError("è‰¹ï¼Œçˆ¬è™«æœªåˆå§‹åŒ–ï¼è¯·ä½¿ç”¨ async with è¯­å¥ã€‚")

        semaphore = asyncio.Semaphore(max_concurrent)

        async def crawl_with_semaphore(url: str) -> dict[str, Any]:
            async with semaphore:
                return await self.crawl(url, config)

        tasks = [crawl_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "success": False,
                    "error": f"ä»»åŠ¡å¼‚å¸¸: {str(result)}",
                })
            else:
                formatted_results.append(result)

        return formatted_results

    async def deep_crawl(
        self,
        url: str,
        strategy: str = "bfs",
        max_pages: int = 10,
        max_depth: int = 3,
        config: Optional[dict[str, Any]] = None,
    ) -> dict[str, Any]:
        """
        æ·±åº¦çˆ¬å–ï¼ˆçˆ¬å–æ•´ä¸ªç½‘ç«™ï¼‰
        Deep crawl (crawl entire website)

        Args:
            url: èµ·å§‹URL
            strategy: çˆ¬å–ç­–ç•¥ï¼ˆbfs/dfsï¼‰
            max_pages: æœ€å¤§é¡µé¢æ•°
            max_depth: æœ€å¤§æ·±åº¦
            config: çˆ¬å–é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            dict: æ·±åº¦çˆ¬å–ç»“æœ
        """
        if not self._crawler:
            raise RuntimeError("è‰¹ï¼Œçˆ¬è™«æœªåˆå§‹åŒ–ï¼è¯·ä½¿ç”¨ async with è¯­å¥ã€‚")

        # è‰¹ï¼Œæ·±åº¦çˆ¬å–éœ€è¦ä½¿ç”¨Crawl4AIçš„æ·±åº¦çˆ¬å–åŠŸèƒ½
        # è¿™é‡Œå…ˆå®ç°åŸºç¡€ç‰ˆæœ¬ï¼Œåç»­å¯ä»¥ä¼˜åŒ–
        visited_urls = set()
        results = []

        async def crawl_recursive(current_url: str, depth: int):
            if depth > max_depth or len(visited_urls) >= max_pages:
                return

            if current_url in visited_urls:
                return

            visited_urls.add(current_url)

            # çˆ¬å–å½“å‰é¡µé¢
            result = await self.crawl(current_url, config)
            results.append({
                "url": current_url,
                "depth": depth,
                "result": result,
            })

            if not result.get("success"):
                return

            # è·å–é“¾æ¥
            links = result.get("links", {}).get("internal", [])

            # BFSæˆ–DFSç­–ç•¥
            if strategy == "bfs":
                # BFSï¼šæŒ‰å±‚çº§çˆ¬å–
                tasks = []
                for link in links[:max_pages]:
                    if link not in visited_urls:
                        tasks.append(crawl_recursive(link, depth + 1))
                await asyncio.gather(*tasks)
            else:
                # DFSï¼šæ·±åº¦ä¼˜å…ˆ
                for link in links[:max_pages]:
                    await crawl_recursive(link, depth + 1)

        await crawl_recursive(url, 0)

        return {
            "success": True,
            "total_pages": len(results),
            "results": results,
        }

    def _build_run_config(self, config: dict[str, Any]) -> CrawlerRunConfig:
        """
        æ„å»ºçˆ¬å–é…ç½®
        Build crawl run configuration

        Args:
            config: é…ç½®å­—å…¸

        Returns:
            CrawlerRunConfig: Crawl4AIè¿è¡Œé…ç½®å¯¹è±¡
        """
        # ç¼“å­˜æ¨¡å¼
        cache_mode = CacheMode.ENABLED
        if config.get("cache_mode") == "bypass":
            cache_mode = CacheMode.BYPASS
        elif config.get("cache_mode") == "disable":
            cache_mode = CacheMode.DISABLED

        # æå–ç­–ç•¥
        extraction_strategy = None
        if config.get("extraction_strategy"):
            # è‰¹ï¼Œè¿™é‡Œå¯ä»¥æ·»åŠ å„ç§æå–ç­–ç•¥
            pass

        # æ„å»ºé…ç½®
        run_config = CrawlerRunConfig(
            cache_mode=cache_mode,
            word_count_threshold=config.get("word_count_threshold", 1),
            extraction_strategy=extraction_strategy,
        )

        return run_config


# ä¾¿æ·å‡½æ•°
async def quick_crawl(
    url: str,
    headless: bool = True,
) -> dict[str, Any]:
    """
    å¿«é€Ÿçˆ¬å–ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    Quick crawl (convenience function)

    Args:
        url: ç›®æ ‡URL
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼

    Returns:
        dict: çˆ¬å–ç»“æœ
    """
    async with Crawl4AIWrapper(headless=headless) as crawler:
        return await crawler.crawl(url)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test():
        """æµ‹è¯•çˆ¬è™«"""
        async with Crawl4AIWrapper() as crawler:
            result = await crawler.crawl("https://example.com")
            print(result)

    asyncio.run(test())
